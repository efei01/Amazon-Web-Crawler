{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f3675-4b72-472e-bd2e-08e439f8e910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cec81-2f57-4b7e-9752-76538992c1b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2744d-707f-48e6-ace1-3acacae2c5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0906bb-7abf-4787-b2a4-68f543223564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5ddad-760c-4176-b11c-f23a4b830d2c",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6575d9-bbec-4249-a1bc-f9256876996b",
   "metadata": {},
   "source": [
    "#### VPN or proxy rotator recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7665c2-e8b0-4ddb-847e-3f6dac16aa74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO) # logger used for debugging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ua = UserAgent() # generate user agents\n",
    "\n",
    "bad_uas = [] # list to hold unreliable user agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf1031-5108-468b-a5bc-22e0977b1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = # number of pages in search results\n",
    "product_links = set() # a set is used to avoid duplicate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9091e-ff0c-4840-95fd-cb4c721aba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = [] # list to hold data scraped from individual product pages\n",
    "faulty_links = [] # list to hold product pages that can't be reached within the number of tries specified by max_retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd4546-d4ef-4007-b54c-26083051e318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_product_list(i, max_retries):\n",
    "    for attempt in range(max_retries):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\") # headless browser\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        user_agent = ua.random # pick random user agent\n",
    "        while user_agent in bad_uas: # ensure user agent isn't a bad one\n",
    "            user_agent = ua.random\n",
    "        chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "        \n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Attempting page {i} with user agent {user_agent} (attempt {attempt+1})\")\n",
    "            driver.get(f'link to product list, use an f string to help denote how the url changes as you go to a new page')\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 's-pagination-strip'))) # dynamic delay while loading page\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            product_list = soup.find_all(attrs={\"data-asin\": re.compile(\"B0.{8}\")})\n",
    "            logger.info(f\"Found {len(product_list)} products on page {i}\")\n",
    "\n",
    "            for product in product_list:\n",
    "                product_link = product.find('a', href=re.compile(\".+B0.+\"))\n",
    "                if product_link is not None and 'amazon.com' not in product_link['href']:\n",
    "                    product_links.add('https://www.amazon.com' + product_link['href'])\n",
    "            \n",
    "            time.sleep(random.uniform(1.0, 5.0)) # random delay after successful scrape and before proceeding to next page\n",
    "            \n",
    "            break # exit loop if successful\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed with user agent {user_agent}: {e}\")\n",
    "            bad_uas.append(user_agent)\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"Max retries reached for page {i}, moving on.\")\n",
    "            time.sleep(random.uniform(1.0, 5.0)) # random delay after failed user-agent and before retry\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae1c71-e2ef-45a8-8b78-c5996b798824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_product_page(link, max_retries, iteration):\n",
    "    for attempt in range(max_retries):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\") # headless browser\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        user_agent = ua.random # pick random user agent\n",
    "        while user_agent in bad_uas: # ensure user agent isn't a bad one\n",
    "            user_agent = ua.random\n",
    "        chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Attempting product #{iteration}, {link}, with user agent {user_agent} (attempt {attempt+1})\")\n",
    "            driver.get(link)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'prodDetails'))) # dynamic delay while loading page\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            \n",
    "            try:\n",
    "                name = soup.find('span', class_='a-size-large product-title-word-break').text.strip()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to find name of {link}: {e}\")\n",
    "                name = None\n",
    "\n",
    "            try:\n",
    "                rating = soup.find('span', class_='a-icon-alt').text.strip()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to find rating of {link}: {e}\")\n",
    "                rating = None\n",
    "                \n",
    "            try:\n",
    "                num_ratings = soup.find('span', id='acrCustomerReviewText').text.strip()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to find the number of ratings of {link}: {e}\")\n",
    "                num_ratings = None\n",
    "            \n",
    "            try:\n",
    "                monthly_purchases = soup.find('span', id='social-proofing-faceout-title-tk_bought').text.strip()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to find the number of monthly purchases of {link}: {e}\")\n",
    "                monthly_purchases = None\n",
    "            \n",
    "            try:\n",
    "                price = soup.find('span', class_='a-price aok-align-center reinventPricePriceToPayMargin priceToPay').text.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    price = soup.find('div', id='corePrice_feature_div').text.strip()\n",
    "                except:\n",
    "                    try:\n",
    "                        price = soup.find('span', class_='a-price a-text-price a-size-medium apexPriceToPay').text.strip()\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to find price of {link}: {e}\")\n",
    "                        price = None\n",
    "            \n",
    "            try:\n",
    "                product_info = soup.find('div', id='productDetails_feature_div').text.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    product_info = soup.find('div', id='detailBulletsWithExceptions_feature_div').text.strip()\n",
    "                except:\n",
    "                    try: \n",
    "                        product_info = soup.find('div', id='prodDetails').text.strip()\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to find product information of {link}: {e}\")\n",
    "                        product_info = None\n",
    "\n",
    "            if product_info != None:\n",
    "                index = product_info.find(\"Warranty & Support\") # remove unnecessary warranty & support section\n",
    "                if index != -1:\n",
    "                    product_info = product_info[:index].strip()\n",
    "                index1 = product_info.find(\"Feedback\") # remove unneccessary feedback section\n",
    "                if index1 != -1:\n",
    "                    product_info = product_info[:index1].strip()\n",
    "            \n",
    "            product = {\n",
    "                'Name': name,\n",
    "                'Rating': rating,\n",
    "                'Number of Ratings': num_ratings,\n",
    "                'Monthly Purchases': monthly_purchases,\n",
    "                'Price': price,\n",
    "                'Product Information': product_info\n",
    "            }\n",
    "            \n",
    "            for key, value in product.items():\n",
    "                if value == '':\n",
    "                    product[key] = None\n",
    "\n",
    "            product_data.append(product)\n",
    "            time.sleep(random.uniform(1.0, 5.0)) # random delay after successful scrape and before proceeding to next link\n",
    "            \n",
    "            break # exit loop if sucessful\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed with user agent {user_agent}: {e}\")\n",
    "            bad_uas.append(user_agent)\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"Max retries reached for {link}, moving on.\")\n",
    "                faulty_links.append(link)\n",
    "            time.sleep(random.uniform(1.0, 5.0)) # random delay after failed user-agent and before retry\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17764c8e-0938-4863-b324-e3a75e7c8507",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scrape product lists\n",
    "\n",
    "try:\n",
    "    for i in range(1, num_pages+1):\n",
    "        scrape_product_list(i, 5)\n",
    "        \n",
    "finally:\n",
    "    logger.info(f\"Total product links found: {len(product_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c945a7-16b2-45c5-bb19-67f441248a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_links = list(product_links) # convert product_links to a list to make it subscriptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db3b48-eb59-41e6-a1e1-7da5a34ef486",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scrape product pages\n",
    "\n",
    "try:\n",
    "    iterations = 1 # track number of products scraped to make debugging easier\n",
    "    for link in product_links:\n",
    "        if iterations % 40 == 0: # reset bad_uas occasionally to avoid running out of reliable user agents, doesn't necessarily have to be every 40 iterations\n",
    "            bad_uas = []\n",
    "        scrape_product_page(link, 3, iterations)\n",
    "        iterations += 1\n",
    "                        \n",
    "finally:\n",
    "    logger.info(f\"Info of {len(product_data)} products found\")\n",
    "    logger.info(product_data[:5])\n",
    "    logger.info(f\"Failed to scrape {len(faulty_links)} links\")\n",
    "    logger.info(faulty_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd4857-7a8e-431f-91bb-72a1f2a9b5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review product data of one product\n",
    "\n",
    "for key, value in product_data[0].items(): \n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b16b5-9aaf-40a6-857c-ee6ad3cbf5a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert product_data to pandas dataframe\n",
    "\n",
    "product_data_df = pd.DataFrame(product_data)\n",
    "product_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef36e6-d700-4812-877b-dcad0c0b9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data_df.isna().sum() # check how many rows of each category are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694ed10-61a7-47b1-bdee-d172370b1d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_data_df.to_csv('name of csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
